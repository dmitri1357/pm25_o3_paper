#!/usr/bin/env python3

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from itertools import compress
from scipy import stats

import matplotlib as mpl
mpl.rc('font',family='Arial')

# load MODIS burned area data
bas = np.load('baf_sums.npy')

# these arrays of PM2.5 and O3 gridded data for the western U.S. can be 
# downloaded from Zenodo.org (see README for details)
# dimension 1: 19 latitudes from 31-49N
# dimension 2: 24 longitudes from 126-103W
# dimension 3: 7300 days from 1 OCT 2000 - 30 SEP 2020
pm_west = np.load('pm_west.npy') # average daily concentrations
oz_west = np.load('oz_west.npy') # MDA8 concentrations

pm_west = np.reshape(pm_west,(456,7300)) # vectorize lats & lons
oz_west = np.reshape(oz_west,(456,7300)) # vectorize lats & lons

year_start = np.arange(0,7300,365)
all_matches = np.empty([456,3660])
all_matches1 = np.empty([456,7300])
for j in range(456):
    cell = pm_west[j,:]
    if np.nanmax(cell) > 0:
        seas_days = np.empty(365)
        fulldays = []
        fulldays1 = []
        for k in range(20):
            step = year_start[k]
            sea = np.arange(step,step+365,1)
            pm2 = pm_west[j,sea]
            oz2 = oz_west[j,sea]
            y3 = np.nanpercentile(pm2,90)
            x3 = np.where(pm2 > y3)[0]
            y4 = np.nanpercentile(oz2,90)
            x4 = np.where(oz2 > y4)[0]
            match2 = [x4[i] in x3 for i in range(len(x4))]
            day_idx2 = list(compress(x4,match2))
            for m in range(365):
                if m not in day_idx2:
                    seas_days[m] = 0
                else:
                    seas_days[m] = 1
            seas_days1 = seas_days[182:]
            fulldays.extend(list(seas_days1))
            fulldays1.extend(list(seas_days))
        all_matches[j,:] = fulldays
        all_matches1[j,:] = fulldays1
    else:
        all_matches[j,:] = ['nan']
        all_matches1[j,:] = ['nan']   
        
match_trunc = all_matches1[:,822:]
all_mats = np.nansum(match_trunc,axis=0)
all_mats = (all_mats/375)*100
maxs = np.flipud(np.argsort(all_mats))

# this code extracts co-occurrence extent peaks from non-overlapping
# 15-day windows between 2003-2020
day_accum = []
for k in range(6478):
    day = maxs[k]
    window = list(np.arange(day-7,day+8,1))
    i, j = window[0], window[-1]
    res = any(ele >= i and ele <= j for ele in day_accum)
    if res == 0:
        day_accum.append(day)
day_accum = np.array(day_accum)

dates = pd.date_range(start='1/1/2003', end='9/30/2020')
dates = dates[~((dates.month == 2) & (dates.day == 29))]
df = pd.DataFrame({'idx':np.arange(0,6478,1),'val':all_mats,'date':dates})
df = df[df['idx'].isin(day_accum)].reset_index()
df['day'] = df.date.dt.date
df1 = df.sort_values(by=['val'], ascending=False).reset_index()
df1['month'] = df1.date.dt.month
df1 = df1[~(df1.month == 10)]
df1 = df1[~(df1.month == 11)]
df1 = df1[~(df1.month == 12)]
df1 = df1[~(df1.month == 1)]
df1 = df1[~(df1.month == 2)]
df1 = df1[~(df1.month == 3)]
df1 = df1[~(df1.month == 4)]
df1 = df1[~(df1.month == 5)]
df1 = df1[~(df1.month == 6)]

df1.drop(columns='level_0',inplace=True)
df1 = df1.reset_index()

vals = []
for k in range(21):
    row = list(df1.values[k,:])
    idx = row[2]
    vals.append(np.round(row[3],1))

lags = np.flipud(np.arange(31))
corrs = []
for j in range(31):
    lag = lags[j]
    ba_maxes = []
    for k in range(21):
        row = list(df1.values[k,:])
        idx = row[2]
        ba_maxes.append(np.nanmax(bas[idx-lag:idx+1]))
    corrs.append(stats.pearsonr(vals[:21],ba_maxes)[0])
    
plt.plot(corrs,'--o')
plt.grid(linewidth=0.5)
plt.yticks(np.arange(0,0.81,0.1), fontsize = 13)
plt.ylabel('Pearson\'s $\it{r}$', fontsize = 13)
plt.xticks(np.arange(0,31,1), ('-30d','','','','','-25d','','','','','-20d',
               '','','','','-15d','','','','','-10d','','','','','-5d','',
               '','','-1d',''), fontsize = 13)
plt.xlabel('Lag (days preceding co-occurrence extent peak)', fontsize = 13)
plt.title('Correlation coefficients between co-occurrence \n extent peaks and lagged burned area peaks', size=15)   
plt.savefig('corr_lags.png',dpi=600)
